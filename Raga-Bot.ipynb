{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xkvi7jGbM8yE",
        "outputId": "28e3650d-09ba-46f0-f6b2-32544e136803"
      },
      "outputs": [],
      "source": [
        "#1 Importing Relevant Libraries\n",
        "import json\n",
        "import string\n",
        "import random \n",
        "\n",
        "import nltk\n",
        "import numpy as np\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "import tensorflow as tf \n",
        "from tensorflow.keras import Sequential \n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"wordnet\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#2 Loading the Dataset: intents.json\n",
        "\n",
        "with open(r'intents.json', encoding='utf-8') as file:\n",
        "    data = json.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GlfhvlVMGMgA"
      },
      "outputs": [],
      "source": [
        "#3 Extracting data_X(features) and data_Y(Target)\n",
        "\n",
        "words = [] #For Bow model/ vocabulary for patterns\n",
        "classes = [] #For Bow  model/ vocabulary for tags\n",
        "data_X = [] #For storing each pattern\n",
        "data_y = [] #For storing tag corresponding to each pattern in data_X \n",
        "# Iterating over all the intents\n",
        "\n",
        "for intent in data[\"intents\"]:\n",
        "    for pattern in intent[\"patterns\"]:\n",
        "        tokens = nltk.word_tokenize(pattern) # tokenize each pattern \n",
        "        words.extend(tokens) #and append tokens to words\n",
        "        data_X.append(pattern) #appending pattern to data_X\n",
        "        data_y.append(intent[\"tag\"]) ,# appending the associated tag to each pattern \n",
        "    \n",
        "    # adding the tag to the classes if it's not there already \n",
        "    if intent[\"tag\"] not in classes:\n",
        "        classes.append(intent[\"tag\"])\n",
        "\n",
        "# initializing lemmatizer to get stem of words\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# lemmatize all the words in the vocab and convert them to lowercase\n",
        "# if the words don't appear in punctuation\n",
        "words = [lemmatizer.lemmatize(word.lower()) for word in words if word not in string.punctuation]\n",
        "# sorting the vocab and classes in alphabetical order and taking the # set to ensure no duplicates occur\n",
        "words = sorted(set(words))\n",
        "classes = sorted(set(classes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxXzyJNNR17c"
      },
      "outputs": [],
      "source": [
        "# 5 Text to Numbers\n",
        "training = []\n",
        "out_empty = [0] * len(classes)\n",
        "# creating the bag of words model\n",
        "for idx, doc in enumerate(data_X):\n",
        "    bow = []\n",
        "    text = lemmatizer.lemmatize(doc.lower())\n",
        "    for word in words:\n",
        "        bow.append(1) if word in text else bow.append(0)\n",
        "    # mark the index of class that the current pattern is associated\n",
        "    # to\n",
        "    output_row = list(out_empty)\n",
        "    output_row[classes.index(data_y[idx])] = 1\n",
        "    # add the one hot encoded BoW and associated classes to training \n",
        "    training.append([bow, output_row])\n",
        "# shuffle the data and convert it to an array\n",
        "random.shuffle(training)\n",
        "training = np.array(training, dtype=object)\n",
        "# split the features and target labels\n",
        "train_X = np.array(list(training[:, 0]))\n",
        "train_Y = np.array(list(training[:, 1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_Q343LiR9w1",
        "outputId": "8eff8feb-d4ab-426a-c091-3530cc69bb6c"
      },
      "outputs": [],
      "source": [
        "#6 The Neural Network Model\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_shape=(len(train_X[0]),), activation=\"relu\"))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation=\"relu\"))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(len(train_Y[0]), activation = \"softmax\"))\n",
        "adam = tf.keras.optimizers.Adam(learning_rate=0.01, decay=1e-6)\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=adam,\n",
        "              metrics=[\"accuracy\"])\n",
        "model.fit(x=train_X, y=train_Y, epochs=150, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjWR6_B7SJ-_"
      },
      "outputs": [],
      "source": [
        "#7 Preprocessing the Input\n",
        "\n",
        "def clean_text(text): \n",
        "  tokens = nltk.word_tokenize(text)\n",
        "  tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "  return tokens\n",
        "\n",
        "def bag_of_words(text, vocab): \n",
        "  tokens = clean_text(text)\n",
        "  bow = [0] * len(vocab)\n",
        "  for w in tokens: \n",
        "    for idx, word in enumerate(vocab):\n",
        "      if word == w: \n",
        "        bow[idx] = 1\n",
        "  return np.array(bow)\n",
        "\n",
        "def pred_class(text, vocab, labels, context, intents):\n",
        "  bow = bag_of_words(text, vocab)\n",
        "  result = model.predict(np.array([bow]))[0]  # Extracting probabilities\n",
        "  thresh = 0.5\n",
        "  y_pred = [[indx, res] for indx, res in enumerate(result) if res > thresh]\n",
        "  y_pred.sort(key=lambda x: x[1], reverse=True)  # Sorting by values of probability in decreasing order\n",
        "  return_list = []\n",
        "  \n",
        "  for r in y_pred:\n",
        "      for intent in intents:\n",
        "          if intent['tag'] == labels[r[0]]:\n",
        "              if intent['context_required'] == \"\" or intent['context_required'] == context:\n",
        "                  return_list.append(labels[r[0]])  # Contains labels(tags) for highest probability \n",
        "\n",
        "  return return_list\n",
        "\n",
        "def get_response(intents_list, intents_json, context):\n",
        "  list_of_intents = intents_json[\"intents\"]\n",
        "  if not intents_list: \n",
        "      tag = 'noanswer'\n",
        "  else:\n",
        "      tag = intents_list[0]\n",
        "\n",
        "  for intent in list_of_intents:\n",
        "      if intent[\"tag\"] == tag:\n",
        "          result = random.choice(intent[\"responses\"])\n",
        "          context = intent[\"context_set\"]  # update context\n",
        "          break\n",
        "  else:\n",
        "      noanswer_tag = [intent for intent in list_of_intents if intent[\"tag\"] == \"noanswer\"]\n",
        "      result = random.choice(noanswer_tag[0][\"responses\"])\n",
        "      context = \"\"  # reset context if noanswer\n",
        "\n",
        "  return result, tag, context\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQLRSslMSQ9s",
        "outputId": "a902746b-a34c-47b2-fb63-8e2643937300"
      },
      "outputs": [],
      "source": [
        "print(\"Persiapkan dirimu untuk olahraga\")\n",
        "\n",
        "# Initialize context\n",
        "context = \"\"\n",
        "\n",
        "# Load intents\n",
        "intents = data[\"intents\"]\n",
        "\n",
        "while True:\n",
        "    message = input(\"\")\n",
        "    intents_list = pred_class(message, words, classes, context, intents)\n",
        "    result, tagged, context = get_response(intents_list, data, context)\n",
        "    print(f'You: {message}')\n",
        "    print(f'Bot: {result}')\n",
        "    if tagged == \"goodbye\":\n",
        "        break"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "ChatBot.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
